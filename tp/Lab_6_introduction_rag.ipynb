{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "In this tutorial, we start with the code you wrote for the starter example of RAG (Retrieval Augmentation Generation) and show you the most common ways you might want to customize it for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TextStreamer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")   \n",
    "from llama_index.core import Settings, PromptTemplate\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace model_name by the one you chose in previous practical exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "device_map = {\"\": torch.cuda.current_device()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "    cache_dir='../models/model/'\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    cache_dir=\"../models/\"\n",
    ")\n",
    "\n",
    "print(f\"{round(model.get_memory_footprint()/1_000_000_000,3)} GB memory used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reminder : Sentence or words embeddings model are mathematical vector representation of content and semantics. Sentence embeddings are particularly useful for document query to cluster, retrieve information or semantic search.\n",
    "\n",
    "Test different sentence embeddings : \n",
    "\n",
    "- [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n",
    "- [dangvantuan/sentence-camembert-base](https://huggingface.co/dangvantuan/sentence-camembert-base)\n",
    "- [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)\n",
    "- [dangvantuan/sentence-camembert-large](https://huggingface.co/dangvantuan/sentence-camembert-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=LangchainEmbedding(\n",
    "    # HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    HuggingFaceEmbeddings(model_name=\"dangvantuan/sentence-camembert-base\")\n",
    "    # HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "    # HuggingFaceEmbeddings(model_name=\"dangvantuan/sentence-camembert-large\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"[INST] <<SYS>>\n",
    "\n",
    "                            Je veux que tu te comportes comme l'assistant documentaire de Michelin.\n",
    "                            L'objectif est de rédiger des paragraphes très qualitatifs et \n",
    "                            pertinents. Ne partage pas de fausses informations.\n",
    "                            Réponds toujours entièrement en français.\n",
    "                            <<SYS>>\n",
    "                            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"Tu trouveras ci-dessous une instruction décrivant une tâche. \"\n",
    "    \"Rédige une réponse qui complète la requête de manière appropriée.\"\n",
    "    \"### Instruction:\\n{query_str}\\n\\n### Réponse:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with [`generate_kwargs` parameters](https://huggingface.co/docs/transformers/v4.18.0/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceLLM(context_window=3500,\n",
    "                    max_new_tokens=200,\n",
    "                    generate_kwargs={\"temperature\" : 0.2,\"pad_token_id\":tokenizer.eos_token_id,\"do_sample\":True,\"repetition_penalty\":1.2},\n",
    "                    system_prompt=system_prompt,\n",
    "                    query_wrapper_prompt=query_wrapper_prompt,\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embeddings\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to give path with your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data_tp6\"\n",
    "documents = SimpleDirectoryReader(data_path).load_data()\n",
    "\n",
    "parser = SentenceSplitter()\n",
    "nodes = parser.get_nodes_from_documents(documents, show_progress=True)\n",
    "\n",
    "\n",
    "for node in nodes : \n",
    "    print(node)\n",
    "    print('--------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine(streaming=True,similarity_top_k=2)\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“I want to parse my documents into smaller chunks”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Settings.chunk_size = 128\n",
    "\n",
    "# Local settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, transformations=[SentenceSplitter(chunk_size=512)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about all integrations available, check out [LlamaHub](https://llamahub.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“I want to use a different response mode”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“I want a chatbot instead of Q&A”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_chat_engine()\n",
    "response = query_engine.chat(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "michelin_formation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
