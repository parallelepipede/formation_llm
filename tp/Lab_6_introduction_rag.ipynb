{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "In this tutorial, we start with the code you wrote for the starter example of RAG (Retrieval Augmentation Generation) and show you the most common ways you might want to customize it for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TextStreamer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")   \n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "device_map = {\"\": torch.cuda.current_device()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "    cache_dir='../models/model/'\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    cache_dir=\"../models/\"\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(name,device_map=device_map,load_in_8bit=True,trust_remote_code=True) #,force_download=True, resume_download=False\n",
    "print(f\"{round(model.get_memory_footprint()/1_000_000_000,3)} GB memory used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=LangchainEmbedding(\n",
    "    # HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    HuggingFaceEmbeddings(model_name=\"dangvantuan/sentence-camembert-base\")\n",
    "    # HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "    # HuggingFaceEmbeddings(model_name=\"dangvantuan/sentence-camembert-large\")\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = model\n",
    "Settings.embed_model = embeddings\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='40c6de45-a08a-4a38-acc7-321c4b396dc0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': 'c:\\\\Users\\\\a812364\\\\OneDrive - Atos\\\\Bureau\\\\Projects\\\\michelin\\\\formation_llm\\\\tp\\\\data_tp6\\\\paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2024-03-12', 'last_modified_date': '2024-03-12'}, hash='d8c7fe5e89e6132678cb68ffddcdb117b6c2687c269d1a4c1fb9e3a8533b69e8'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6935a8dd-82cb-4515-b70f-e6f379062247', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='a86dcfe59f6bd34948c27f71ce3559236525c77d68fd0be377e2548e308f9ec0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"data_tp6\").load_data()\n",
    "\n",
    "parser = SentenceSplitter()\n",
    "nodes = parser.get_nodes_from_documents(documents, show_progress=True)\n",
    "\n",
    "\n",
    "for node in nodes : \n",
    "    print(node)\n",
    "    print('--------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine(streaming=True,similarity_top_k=2)\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“I want to parse my documents into smaller chunks”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Settings.chunk_size = 128\n",
    "\n",
    "# Local settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, transformations=[SentenceSplitter(chunk_size=512)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about all integrations available, check out [LlamaHub](https://llamahub.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“I want to use a different response mode”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query_engine = index.as_query_engine(response_mode=\"tree_summarize\")\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“I want a chatbot instead of Q&A”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_chat_engine()\n",
    "response = query_engine.chat(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pour aller plus loin\n",
    "\n",
    "Définir un PromptTemplate\n",
    "Définir un SystemPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"[INST] <<SYS>>\n",
    "\n",
    "                            Je veux que tu te comportes comme l'assistant documentaire de Michelin.\n",
    "                            L'objectif est de rédiger des paragraphes très qualitatifs et \n",
    "                            pertinents. Ne partage pas de fausses informations.\n",
    "                            Réponds toujours entièrement en français.\n",
    "                            <<SYS>>\n",
    "                            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"Below is an instruction that describes a task. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{query_str}\\n\\n### Response:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceLLM(context_window=3500,\n",
    "                    max_new_tokens=200,\n",
    "                    generate_kwargs={\"temperature\" : 0.2,\"pad_token_id\":tokenizer.eos_token_id,\"do_sample\":True,\"repetition_penalty\":1.2},\n",
    "                    system_prompt=system_prompt,\n",
    "                    query_wrapper_prompt=query_wrapper_prompt,\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embeddings\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import SemanticSimilarityEvaluator\n",
    "import nest_asyncio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "michelin_formation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
